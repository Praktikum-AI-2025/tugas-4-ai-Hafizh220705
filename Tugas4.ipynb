{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hNi7Bq07TDPY"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9BvWvE2yTf7C"
      },
      "outputs": [],
      "source": [
        "# 1. Download and extract the dataset\n",
        "def download_and_extract_data():\n",
        "    # Download horse-or-human dataset\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    with zipfile.ZipFile('horse-or-human.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data/horse-or-human')\n",
        "\n",
        "    # Download validation-horse-or-human dataset\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    with zipfile.ZipFile('validation-horse-or-human.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data/validation-horse-or-human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EtA8n41tTknP"
      },
      "outputs": [],
      "source": [
        "# 2. Prepare Image Data Generators for training and validation\n",
        "def prepare_data():\n",
        "    # Define directories for training and validation\n",
        "    TRAINING_DIR = 'data/horse-or-human'\n",
        "    VALIDATION_DIR = 'data/validation-horse-or-human'\n",
        "\n",
        "    # Image data generator for training with augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Image data generator for validation\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Flow images from directories\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        TRAINING_DIR,\n",
        "        target_size=(150, 150),  # Resize images to 150x150\n",
        "        batch_size=32,\n",
        "        class_mode='binary'  # Binary classification: Horse or Human\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        VALIDATION_DIR,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7cNlyOfiTn9J"
      },
      "outputs": [],
      "source": [
        "# 3. Build the CNN model\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        # First convolutional layer with 16 filters and 3x3 kernel size\n",
        "        Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        # Second convolutional layer with 32 filters and 3x3 kernel size\n",
        "        Conv2D(32, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        # Third convolutional layer with 64 filters and 3x3 kernel size\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "\n",
        "        # Flatten the output from convolutional layers\n",
        "        Flatten(),\n",
        "\n",
        "        # Fully connected layer with 512 neurons\n",
        "        Dense(512, activation='relu'),\n",
        "\n",
        "        # Output layer with 1 neuron and sigmoid activation for binary classification\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model using RMSprop optimizer and binary crossentropy loss\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=RMSprop(learning_rate=0.001),  # Corrected here\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0xxUTjqYTqu5"
      },
      "outputs": [],
      "source": [
        "# 4. Train the model\n",
        "def train_model(model, train_generator, validation_generator):\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=100,  # Number of batches per epoch\n",
        "        epochs=10,  # Number of epochs\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=50  # Number of validation steps\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxOEGdGaTuIF",
        "outputId": "4d193554-dbbf-4314-ac8c-4cc4e86f30c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 33/100\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 1s/step - accuracy: 0.5634 - loss: 0.9033"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 382ms/step - accuracy: 0.6250 - loss: 0.7811 - val_accuracy: 0.7773 - val_loss: 0.4761\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 373ms/step - accuracy: 0.8324 - loss: 0.4108 - val_accuracy: 0.7070 - val_loss: 1.6774\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 370ms/step - accuracy: 0.8876 - loss: 0.2708 - val_accuracy: 0.6836 - val_loss: 1.8937\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 375ms/step - accuracy: 0.9052 - loss: 0.2659 - val_accuracy: 0.6641 - val_loss: 2.2266\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 365ms/step - accuracy: 0.9134 - loss: 0.2537 - val_accuracy: 0.7227 - val_loss: 1.5135\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 366ms/step - accuracy: 0.9387 - loss: 0.1525 - val_accuracy: 0.7109 - val_loss: 1.9848\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 406ms/step - accuracy: 0.9244 - loss: 0.2036 - val_accuracy: 0.7305 - val_loss: 1.4489\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 363ms/step - accuracy: 0.9619 - loss: 0.1099 - val_accuracy: 0.6953 - val_loss: 1.5759\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 364ms/step - accuracy: 0.9539 - loss: 0.1405 - val_accuracy: 0.8164 - val_loss: 1.1129\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 368ms/step - accuracy: 0.9274 - loss: 0.2381 - val_accuracy: 0.6211 - val_loss: 3.8764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9426\n",
            "Validation Accuracy: 0.6211\n"
          ]
        }
      ],
      "source": [
        "# 5. Main function to run the complete workflow\n",
        "def main():\n",
        "    # Download and extract data\n",
        "    download_and_extract_data()\n",
        "\n",
        "    # Prepare the data generators\n",
        "    train_generator, validation_generator = prepare_data()\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = train_model(model, train_generator, validation_generator)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('horse_or_human_model.h5')\n",
        "\n",
        "    # Output training and validation accuracy\n",
        "    print(f\"Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
